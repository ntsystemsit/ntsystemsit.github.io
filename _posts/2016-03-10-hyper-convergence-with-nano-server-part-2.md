---
layout: post
title: "Hyper-Convergence with Nano Server: Part 2"
date: 2016-03-10 21:25:00 +0100
comments: true
published: true
excerpt_separator: <!-- more -->
tags: ["Server", "en"]
redirect_from: ["/post/Hyper-Convergence-with-Nano-Server-Part-2", "/post/hyper-convergence-with-nano-server-part-2"]
---
<!-- more -->
{% include imported_disclaimer.html %}
<p>In this post, we build on the Nano Servers that were set-up in the <a href="/post/Hyper-Convergence-with-Nano-Server-Part-1.aspx" target="_blank">part one</a>, we are going to create a Failover Cluster and enable Storage Spaces Direct.</p> <h1></h1> <h1>Cluster Network</h1> <p>When creating the Nano Server vhd files, I did add the package for failover clustering by using the –Clustering parameter of New-NanoServerImage. So now that the servers are running, I can use remote server management tools or PowerShell to create a cluster. </p> <p>But, before doing that, I went ahead and configured the second network adapter with static IP addressing. Remember that I created the Nano VMs with two network adapters, one connected to my lab network – where DHCP provides addressing – the other connected to an internal vSwitch, as it’ll be used for cluster communication only.</p> <p><script src="https://gist.github.com/tomtorggler/c045d8c9da7b74fb84e0.js"></script></p> <h1>Failover Cluster</h1> <p>Before creating a cluster I run the “Cluster Validation Wizard” to make sure my configuration checks out. We could run that wizard using the GUI tools or using the following line of PowerShell:</p> <p><code>Test-Cluster –Node n01,n02,n03,n04&nbsp; –Include “Storage Spaces Direct”,Inventory,Network,”System Configuration”</code></p> <p>Some summary information as well as the location of the html report will be printed to the screen. In the report we can actually see the results of the tests that were run. Problems are pointed out and some level of detail is provided.</p> <p><a href="/assets/image_694.png"><img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="/assets/image_thumb_692.png" width="244" height="74"></a></p> <p>The only warning I got was the following: Failed to get SCSI page 83h VPD descriptors for physical disk 2. This was because my VMs originally used the default “LSI Logic SAS” controller for the OS disk, while the additional, 100GB disks were connected to the SATA Controller. To fix this, I did connect the OS disk to the SATA Controller as well.</p> <p>All green, so we can go ahead and create the cluster:</p> <p><code>New-Cluster -Name hc01 -Node n01,n02,n03,n04 –NoStorage</code></p> <p>This will create another html report at a temporary location, the path will again be printed to the screen. My cluster has an even number of nodes, so I decided to use a File Share Witness. Using the following commands, I created a quick share on my management server (tp4):</p> <p><code>New-Item -ItemType Directory -Path c:\hc01witness <br>New-FileShare -Name hc01witness -RelativePathName hc01witness -SourceVolume (Get-Volume c) -FileServerFriendlyName tp4<br>Get-FileShare -Name hc01witness | Grant-FileShareAccess -AccountName everyone -AccessRight Full</code></p> <p>After that, I used the following to update the cluster quorum configuration:</p> <p><code>Set-ClusterQuorum -Cluster hc01 -FileShareWitness \\tp4.vdi.local\hc01witness </code></p> <h1>Storage Spaces Direct</h1> <p>At this point, we have a running cluster and we can go ahead and configure storage spaces.</p> <p><a href="/assets/image_695.png"><img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="/assets/image_thumb_693.png" width="244" height="158"></a></p> <p>But again, before doing that, I want to point something out. If we open the Failover Cluster Manager at this point, we can connect to the cluster just as we are used to. If we expand the “Storage” folder in the newly created cluster, we can see there are no Disks, no Pools and no Enclosures present at this time.</p> <p>Using the following command, I enabled storage spaces direct for the cluster:</p> <p><code>Enable-ClusterS2D -S2DCacheMode Disabled -Cluster hc01</code></p> <p>And using the following command, I create a new storage pool using the “Clustered Windows Storage” subsystem:<br></p> <p><script src="https://gist.github.com/tomtorggler/2948c5a905f0e1e176a6.js"></script></p> <p>If we go back to the Failover Cluster Manager after the above steps, we can see that now there is a Pool as well as four Enclosures, one representing each server:</p> <p><a href="/assets/image_696.png"><img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="/assets/image_thumb_694.png" width="244" height="54"></a></p> <p>Ok, so far it is looking quite good, and we can go ahead an create a volume (or virtual disk):</p> <p><script src="https://gist.github.com/tomtorggler/e833d53542d1ff5640ec.js"></script></p> <p>Our new virtual disk will automatically be added to the Cluster Shared volumes, we can verify that using:</p> <p><code>Get-ClusterSharedVolume -Cluster hc01</code></p> <p>Alright, we are getting close. In the next post, we are going to configure the Hyper-V roles and create some virtual machines. Sounds great? Stay tuned :)</p> <p>&nbsp;</p> <p>Cheers,<br>Tom</p>
